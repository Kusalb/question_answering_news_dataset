{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6730fecf",
   "metadata": {},
   "source": [
    "# Assignment 2 \n",
    "### Kusal Bista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41730de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.16\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb617560-ebfe-47a0-a5ee-fb6a0ec56589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for data reading\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "import neuralcoref\n",
    "import stanza\n",
    "# Libraries for pre-processing\n",
    "import re\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Libraries for information retrieval\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.lang.en import English\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Libraries for data analysis\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Libraries for question answering\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "import torch\n",
    "import time\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "858f5298-3839-41d9-a86d-2947fc751b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8771dd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20cde6e7-8e19-4357-8ade-60114da4174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download en_core_web_md\n",
    "# !pip install tabulate\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d0d6e5f-384d-47c7-b959-51604a6bc35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/poojakc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/poojakc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/poojakc/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/poojakc/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd8a509-a9f1-405e-8d0b-6dc96e679322",
   "metadata": {},
   "source": [
    "### 1 Reading dataset and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1474a82-79ad-4c8d-8bae-e4d4056d2860",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = pd.read_csv('news_dataset.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de28f0fb-9811-4b2d-96f4-63ff46825a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>topic</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17307</td>\n",
       "      <td>Marlise Simons</td>\n",
       "      <td>1/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>architecture</td>\n",
       "      <td>PARIS  ?   When the Islamic State was about to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17292</td>\n",
       "      <td>Andy Newman</td>\n",
       "      <td>31/12/2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>art</td>\n",
       "      <td>Angels are everywhere in the Mu?iz family?s ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17298</td>\n",
       "      <td>Emma G. Fitzsimmons</td>\n",
       "      <td>2/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>Finally. The Second Avenue subway opened in Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17311</td>\n",
       "      <td>Carl Hulse</td>\n",
       "      <td>3/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>WASHINGTON  ?   It?s   or   time for Republica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17339</td>\n",
       "      <td>Jim Rutenberg</td>\n",
       "      <td>5/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>For Megyn Kelly, the shift from Fox News to NB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id               author        date  year month         topic  \\\n",
       "0  17307       Marlise Simons   1/01/2017  2017     1  architecture   \n",
       "1  17292          Andy Newman  31/12/2016  2016    12           art   \n",
       "2  17298  Emma G. Fitzsimmons   2/01/2017  2017     1      business   \n",
       "3  17311           Carl Hulse   3/01/2017  2017     1      business   \n",
       "4  17339        Jim Rutenberg   5/01/2017  2017     1      business   \n",
       "\n",
       "                                             article  \n",
       "0  PARIS  ?   When the Islamic State was about to...  \n",
       "1  Angels are everywhere in the Mu?iz family?s ap...  \n",
       "2  Finally. The Second Avenue subway opened in Ne...  \n",
       "3  WASHINGTON  ?   It?s   or   time for Republica...  \n",
       "4  For Megyn Kelly, the shift from Fox News to NB...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e04327fa-2d19-4ba8-9291-984303ca5ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled dataset shape: (100, 7)\n"
     ]
    }
   ],
   "source": [
    "# selecting 100 sample \n",
    "test_article = news_dataset[news_dataset['id'] == 17574]\n",
    "sample_size = 100\n",
    "if news_dataset.shape[0] >= sample_size:\n",
    "    news_dataset = news_dataset.sample(n=sample_size, random_state=42)  # Adjusting random_state for reproducibility\n",
    "    news_dataset.reset_index(drop=True, inplace=True) \n",
    "    print(\"Sampled dataset shape:\", news_dataset.shape)\n",
    "else:\n",
    "    print(\"Dataset size is less than the sample size. Cannot perform sampling.\")\n",
    "    \n",
    "news_dataset = pd.concat([news_dataset, test_article], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "573c3a0c-0f0f-4790-a794-9a0f87613737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101 entries, 0 to 100\n",
      "Data columns (total 7 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   id       101 non-null    int64 \n",
      " 1   author   100 non-null    object\n",
      " 2   date     101 non-null    object\n",
      " 3   year     101 non-null    object\n",
      " 4   month    101 non-null    object\n",
      " 5   topic    101 non-null    object\n",
      " 6   article  101 non-null    object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 5.6+ KB\n"
     ]
    }
   ],
   "source": [
    "news_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b177652d-42ca-4e6f-ba8d-ad3a95c503b1",
   "metadata": {},
   "source": [
    "### 1.2 Handling missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be0eccc9-7ea3-4155-be46-4373e2338b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing value\n",
      "id         0\n",
      "author     1\n",
      "date       0\n",
      "year       0\n",
      "month      0\n",
      "topic      0\n",
      "article    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing value\")\n",
    "print(news_dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf22242e-51d4-48a3-b91d-58d55ca2216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing value\n",
    "news_dataset['author'] = news_dataset['author'].fillna('No author')\n",
    "# checking missing value after handling missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71e52309-3c2c-4899-b621-263597321d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After handling missing value\n",
      "id         0\n",
      "author     0\n",
      "date       0\n",
      "year       0\n",
      "month      0\n",
      "topic      0\n",
      "article    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"After handling missing value\")\n",
    "print(news_dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edb4773a-7d99-41fe-bed4-f10628d76eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>topic</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17663</td>\n",
       "      <td>Hannah Berkeley Cohen, Azam Ahmed and Frances ...</td>\n",
       "      <td>14/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>HAVANA  ?   Andr?s Ivÿn and his girlfriend gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17361</td>\n",
       "      <td>No author</td>\n",
       "      <td>15/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>For the 12th straight year, the Travel section...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18058</td>\n",
       "      <td>Rana F. Sweis</td>\n",
       "      <td>29/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>MAFRAQ, Jordan  ?   Nisreen   thought the wors...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18315</td>\n",
       "      <td>Andrew Higgins</td>\n",
       "      <td>12/02/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>politics</td>\n",
       "      <td>NICOSIA, Cyprus  ?   As the United Nations gea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17612</td>\n",
       "      <td>Jennifer Senior</td>\n",
       "      <td>12/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "      <td>Let?s just get this out of the way, shall we? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17294</td>\n",
       "      <td>John Schwartz</td>\n",
       "      <td>5/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>science</td>\n",
       "      <td>THOMPSONS, Tex.  ?   Can one of the most promi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17728</td>\n",
       "      <td>Nelson D. Schwartz and Bill Vlasic</td>\n",
       "      <td>20/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>Donald J. Trump won?t be sworn in until Friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17637</td>\n",
       "      <td>Mike Hale</td>\n",
       "      <td>14/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>?Homeland,? a series always conscious of curre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>18253</td>\n",
       "      <td>Shivani Vora</td>\n",
       "      <td>14/02/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Beach and spa vacations are typically associat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17904</td>\n",
       "      <td>Brooks Barnes</td>\n",
       "      <td>23/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Nominations for the 89th Academy Awards will b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>18196</td>\n",
       "      <td>Emily Palmer</td>\n",
       "      <td>2/02/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>crime</td>\n",
       "      <td>After an explosive fight with her boyfriend, M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17698</td>\n",
       "      <td>Jim Rutenberg</td>\n",
       "      <td>16/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "      <td>BEVERLY HILLS, Calif.  ?   It was Nov. 12, and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                             author        date  \\\n",
       "0   17663  Hannah Berkeley Cohen, Azam Ahmed and Frances ...  14/01/2017   \n",
       "1   17361                                          No author  15/01/2017   \n",
       "2   18058                                      Rana F. Sweis  29/01/2017   \n",
       "3   18315                                     Andrew Higgins  12/02/2017   \n",
       "4   17612                                    Jennifer Senior  12/01/2017   \n",
       "5   17294                                      John Schwartz   5/01/2017   \n",
       "6   17728                 Nelson D. Schwartz and Bill Vlasic  20/01/2017   \n",
       "7   17637                                          Mike Hale  14/01/2017   \n",
       "8   18253                                       Shivani Vora  14/02/2017   \n",
       "9   17904                                      Brooks Barnes  23/01/2017   \n",
       "10  18196                                       Emily Palmer   2/02/2017   \n",
       "11  17698                                      Jim Rutenberg  16/01/2017   \n",
       "\n",
       "    year month          topic  \\\n",
       "0   2017     1       business   \n",
       "1   2017     1      lifestyle   \n",
       "2   2017     1  entertainment   \n",
       "3   2017     2       politics   \n",
       "4   2017     1       politics   \n",
       "5   2017     1        science   \n",
       "6   2017     1       business   \n",
       "7   2017     1       business   \n",
       "8   2017     2  entertainment   \n",
       "9   2017     1  entertainment   \n",
       "10  2017     2          crime   \n",
       "11  2017     1       politics   \n",
       "\n",
       "                                              article  \n",
       "0   HAVANA  ?   Andr?s Ivÿn and his girlfriend gre...  \n",
       "1   For the 12th straight year, the Travel section...  \n",
       "2   MAFRAQ, Jordan  ?   Nisreen   thought the wors...  \n",
       "3   NICOSIA, Cyprus  ?   As the United Nations gea...  \n",
       "4   Let?s just get this out of the way, shall we? ...  \n",
       "5   THOMPSONS, Tex.  ?   Can one of the most promi...  \n",
       "6   Donald J. Trump won?t be sworn in until Friday...  \n",
       "7   ?Homeland,? a series always conscious of curre...  \n",
       "8   Beach and spa vacations are typically associat...  \n",
       "9   Nominations for the 89th Academy Awards will b...  \n",
       "10  After an explosive fight with her boyfriend, M...  \n",
       "11  BEVERLY HILLS, Calif.  ?   It was Nov. 12, and...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset = news_dataset.drop_duplicates(subset=['article'], keep='first').reset_index(drop=True)\n",
    "news_dataset.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcbbed6-f57f-4686-891e-f2b9d6466935",
   "metadata": {},
   "source": [
    "### 1.3 Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e2c0aee-9743-46fc-8810-c8072892a0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "\n",
    "def pre_process(data):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update([\"This\", \"The\", \"the\"])\n",
    "    s = \" \\[(?=.*\\d).*?\\]\" \n",
    "    # Lemmatization and removal of stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#     print(data)\n",
    "    result = []\n",
    "    for text in data:\n",
    "        doc = nlp(text)\n",
    "        text = doc._.coref_resolved\n",
    "        # Clean text\n",
    "        # Remove non-ASCII characters\n",
    "        text = ''.join([char for char in text if ord(char) < 128])\n",
    "        # Remove multiple spaces\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Remove question mark problems\n",
    "        text = re.sub(r'(\\s\\?)',' ',text)\n",
    "        text = re.sub(r\"\\b\\?\\b\", \"\\'\", text)\n",
    "        text = re.sub(r\"(,\\?)\",\",\", text)\n",
    "        text = re.sub(r\"\\?+\", \"?\", text)\n",
    "        text = text.strip()\n",
    "        # Lemmatization and removal of stopwords\n",
    "        processed_text = \" \".join([lemmatizer.lemmatize(word) for word in re.sub(s, \"\", text).split() if word.lower() not in stop_words])\n",
    "        result.append(processed_text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d773dd50-b4ea-441c-97ea-d077c2a98e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset['processed_article'] = pre_process(news_dataset['article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "820cfb36-4b63-4def-be79-4844cbec48a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>topic</th>\n",
       "      <th>article</th>\n",
       "      <th>processed_article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17904</td>\n",
       "      <td>Brooks Barnes</td>\n",
       "      <td>23/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Nominations for the 89th Academy Awards will b...</td>\n",
       "      <td>Nominations 89th Academy Awards announced Tues...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18166</td>\n",
       "      <td>Matt Flegenheimer</td>\n",
       "      <td>2/02/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>business</td>\n",
       "      <td>WASHINGTON  ?   President Trump, seeming to re...</td>\n",
       "      <td>WASHINGTON Mr. Trump, encouraged Mitch, Wednes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18169</td>\n",
       "      <td>Somini Sengupta</td>\n",
       "      <td>2/02/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>business</td>\n",
       "      <td>UNITED NATIONS  ?   The new secretary general ...</td>\n",
       "      <td>UNITED NATIONS new secretary general United Na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18055</td>\n",
       "      <td>Emily Palmer</td>\n",
       "      <td>26/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>On a chilly October morning, Talea Childs, 4, ...</td>\n",
       "      <td>chilly October morning, Talea Childs, 4, still...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17776</td>\n",
       "      <td>Liam Stack</td>\n",
       "      <td>20/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>celebrities may be staying away from Donald J....</td>\n",
       "      <td>celebrity may staying away Donald J. Trump's i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id             author        date  year month          topic  \\\n",
       "0  17904      Brooks Barnes  23/01/2017  2017     1  entertainment   \n",
       "1  18166  Matt Flegenheimer   2/02/2017  2017     2       business   \n",
       "2  18169    Somini Sengupta   2/02/2017  2017     2       business   \n",
       "3  18055       Emily Palmer  26/01/2017  2017     1      lifestyle   \n",
       "4  17776         Liam Stack  20/01/2017  2017     1  entertainment   \n",
       "\n",
       "                                             article  \\\n",
       "0  Nominations for the 89th Academy Awards will b...   \n",
       "1  WASHINGTON  ?   President Trump, seeming to re...   \n",
       "2  UNITED NATIONS  ?   The new secretary general ...   \n",
       "3  On a chilly October morning, Talea Childs, 4, ...   \n",
       "4  celebrities may be staying away from Donald J....   \n",
       "\n",
       "                                   processed_article  \n",
       "0  Nominations 89th Academy Awards announced Tues...  \n",
       "1  WASHINGTON Mr. Trump, encouraged Mitch, Wednes...  \n",
       "2  UNITED NATIONS new secretary general United Na...  \n",
       "3  chilly October morning, Talea Childs, 4, still...  \n",
       "4  celebrity may staying away Donald J. Trump's i...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a344fceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173a041b-82b8-4302-908a-45d6bc6b4774",
   "metadata": {},
   "source": [
    "#### 2 Classical retrivel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bcb4141-bd9c-476b-9b6b-0c392fb41dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_md = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "class TextMatchingUtility:\n",
    "    def __init__(self, dataset):\n",
    "        self.data = dataset  # Dataset\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def preprocess_query(self, query):\n",
    "        # Regular expression to match text patterns\n",
    "        s = \" \\[(?=.*\\d).*?\\]\"\n",
    "        # Removing stopwords and Lemmatization\n",
    "        stop_words = stopwords.words('english')\n",
    "        stop_words.extend([\"This\", \"The\", \"the\"])\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        result = [\" \".join([lemmatizer.lemmatize(word) for word in re.sub(s, \"\", query).split() if word not in stop_words])]\n",
    "        return result\n",
    "\n",
    "\n",
    "    def tf_idf_score(self, query, articles):    \n",
    "        vectorizer = TfidfVectorizer()\n",
    "        # Convert to word vector\n",
    "        articles_wv = vectorizer.fit_transform(articles)\n",
    "        # Convert to word vector\n",
    "        query_wv = vectorizer.transform([query]) \n",
    "        # Calculate similarity\n",
    "        similarities = cosine_similarity(query_wv, articles_wv)[0]\n",
    "        return similarities\n",
    "\n",
    "    def spacy_score(self, query, articles):\n",
    "        # Convert to word vector\n",
    "        query_nlp = nlp_md(str(query))\n",
    "        # Convert to word vector\n",
    "        articles_nlp = [nlp_md(article) for article in articles]\n",
    "        # Calculate similarity\n",
    "        similarities = [query_nlp.similarity(article_nlp) for article_nlp in articles_nlp]\n",
    "        return similarities\n",
    "    \n",
    "    def get_best_sentences(self, query, article_id, word_vector, top_n=3):\n",
    "        article = self.data.loc[self.data['id'] == article_id, 'processed_article'].iloc[0]\n",
    "        # Convert text into sentences\n",
    "        sentences_clean = tokenize.sent_tokenize(article)\n",
    "        # Calculate similarity\n",
    "        if word_vector == \"tf-idf\":\n",
    "            similarities = self.tf_idf_score(query, sentences_clean)\n",
    "        elif word_vector == \"spaCy\":\n",
    "            similarities = self.spacy_score(query, sentences_clean)\n",
    "        # Get the indices of top N scores\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_n]\n",
    "        # Get the top N sentences and their scores\n",
    "        top_sentences = [(sentences_clean[i], similarities[i]) for i in top_indices]\n",
    "        return top_sentences \n",
    "\n",
    "    def get_best_sentence(self, query, article_id, word_vector):\n",
    "        article = self.data.loc[self.data['id'] == article_id, 'processed_article'].iloc[0]\n",
    "        # Convert text into sentences\n",
    "        sentences_clean = tokenize.sent_tokenize(article)\n",
    "        # Calculate similarity\n",
    "        if word_vector == \"tf-idf\":\n",
    "            similarities = self.tf_idf_score(query, sentences_clean)\n",
    "        elif word_vector == \"spaCy\":\n",
    "            similarities = self.spacy_score(query, sentences_clean)\n",
    "    \n",
    "        # Get the maximum score index position\n",
    "        best_idx = np.array(similarities).argmax()\n",
    "        # Get the best score\n",
    "        best_score = max(similarities)\n",
    "        # Get original data\n",
    "        for j in range(len(self.data['id'])):\n",
    "            if self.data['id'][j] == article_id:\n",
    "                topic = self.data['article'][j]\n",
    "        sentences_topic = tokenize.sent_tokenize(topic)\n",
    "        answer = sentences_topic[best_idx]\n",
    "        print(\"Article ID \", article_id)  \n",
    "        print(\"Question:\", query)\n",
    "        if best_score < 0.3:\n",
    "            print(\"No answer found\")\n",
    "        else:\n",
    "            print(\"Answer:\", answer)\n",
    "            print(\"Score\", best_score,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180cbb60-87e5-4a14-9d21-b3878e6f58bf",
   "metadata": {},
   "source": [
    "### 2.1 tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9a6d3ac-7791-4227-8f55-5b7d496fc184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is the vice president of Samsung?\n",
      "Answer 1: special prosecutor's office said special prosecutor's office evidence Mr. Lee received request bribery president ordered Samsung subsidiary send bribe destination designated president.\n",
      "Score: 0.3697214925441742\n",
      "\n",
      "Answer 2: SEOUL, South Korea special prosecutor investigating corruption scandal led President Park impeachment summoned de facto head Samsung questioning Wednesday, calling bribery suspect.\n",
      "Score: 0.21046949811335275\n",
      "\n",
      "Answer 3: Mr. Lee expected asked whether donation Samsung made two foundation controlled Choi longtime friend president, amounted bribes, role, any, Mr. Lee played decision give money.\n",
      "Score: 0.19505615219055755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tm_utility = TextMatchingUtility(news_dataset)\n",
    "\n",
    "# Sample question\n",
    "question = \"Who is the vice president of Samsung?\"\n",
    "article_id = 17574\n",
    "word_vector = 'tf-idf'\n",
    "\n",
    "top_results = tm_utility.get_best_sentences(question, article_id, word_vector, top_n=3)\n",
    "\n",
    "print(\"Question:\", question)\n",
    "for i, (answer, score) in enumerate(top_results, 1):\n",
    "    print(f\"Answer {i}: {answer}\")\n",
    "    print(\"Score:\", score)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e91057-d03f-464c-b23d-7644b688913a",
   "metadata": {},
   "source": [
    "### 2.2 tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f99598df-8b99-466c-83cc-95a86618fdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is the vice chairman of Samsung?\n",
      "Answer 1: special prosecutor's office said special prosecutor's office evidence Mr. Lee received request bribery president ordered Samsung subsidiary send bribe destination designated president.\n",
      "Score: 0.8260947373005356\n",
      "\n",
      "Answer 2: Mr. Lee expected asked whether donation Samsung made two foundation controlled Choi longtime friend president, amounted bribes, role, any, Mr. Lee played decision give money.\n",
      "Score: 0.8260032233848222\n",
      "\n",
      "Answer 3: national pension fund's support crucial merger, analyst said helped Mr. Lee inherit control Samsung Mr. Lee father.\n",
      "Score: 0.7937786586865135\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tm_utility = TextMatchingUtility(news_dataset)\n",
    "\n",
    "# Sample question\n",
    "question = \"Who is the vice chairman of Samsung?\"\n",
    "article_id = 17574\n",
    "word_vector = 'spaCy'\n",
    "\n",
    "top_results = tm_utility.get_best_sentences(question, article_id, word_vector, top_n=3)\n",
    "\n",
    "print(\"Question:\", question)\n",
    "for i, (answer, score) in enumerate(top_results, 1):\n",
    "    print(f\"Answer {i}: {answer}\")\n",
    "    print(\"Score:\", score)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa8c3bc-ae47-418c-9cf8-ce3a9b4c548d",
   "metadata": {},
   "source": [
    "### Sample Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f811a17c-cfd6-4a31-8af1-c0debaf1b335",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Test questions\n",
    "test_questions = [\n",
    "    {'query': 'Who is the vice chairman of Samsung?', 'answer': 'Jay Y. Lee'},\n",
    "    {'query': 'Who is the de facto head of Samsung being questioned for bribery?', 'answer': 'Jay Y. Lee'},\n",
    "    {'query': 'What scandal led to President Park\\'s impeachment?', 'answer': 'corruption scandal'},\n",
    "    {'query': 'What is the name of Samsung\\'s vice chairman?', 'answer': 'Jay Y. Lee'},\n",
    "    {'query': 'What is the name of the special prosecutor investigating the corruption scandal?', 'answer': '[SEP]'},\n",
    "    {'query': 'Who is on trial at the Constitutional Court?', 'answer': 'Ms. Park'},\n",
    "    {'query': 'What is the name of the special prosecutor\\'s office spokesman?', 'answer': 'Lee'},\n",
    "    {'query': 'What charges were filed against Ms. Choi by state prosecutors?', 'answer': 'coercing 53 big businesses'},\n",
    "    {'query': 'What organization\\'s support was crucial for the merger of two Samsung affiliates?', 'answer': 'National Pension Service'},\n",
    "    {'query': 'What amount did Samsung contribute to Ms. Choi\\'s winter sports program?', 'answer': '$1.3 million'}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95ba4fc0-1ba8-48a1-b172-20381d0b1762",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestUtility:\n",
    "    def __init__(self, text_matching_utility, test_questions):\n",
    "        self.text_matching_utility = text_matching_utility\n",
    "        self.test_questions = test_questions\n",
    "\n",
    "    def evaluate_mrr(self, article_id, word_vector):\n",
    "        reciprocal_ranks = []\n",
    "\n",
    "        for question_data in self.test_questions:\n",
    "            query = question_data['query']\n",
    "            true_answer = question_data['answer']\n",
    "\n",
    "            # Get the best sentence from the text matching utility\n",
    "            best_sentence = self.text_matching_utility.get_best_sentence(query, article_id, word_vector)\n",
    "\n",
    "            # If no answer found, skip this question\n",
    "            if not best_sentence:\n",
    "                continue\n",
    "\n",
    "            # Check if the true answer is in the best sentence\n",
    "            if true_answer in best_sentence:\n",
    "                rank = best_sentence.index(true_answer) + 1  # Rank of the true answer\n",
    "                reciprocal_ranks.append(1 / rank)\n",
    "\n",
    "        # Calculate MRR\n",
    "        mrr = sum(reciprocal_ranks) / len(reciprocal_ranks) if reciprocal_ranks else 0\n",
    "        return mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5beb52f1-45c9-44cf-8431-c629794a8ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article ID  17574\n",
      "Question: Who is the vice chairman of Samsung?\n",
      "No answer found\n",
      "Article ID  17574\n",
      "Question: Who is the de facto head of Samsung being questioned for bribery?\n",
      "Answer: SEOUL, South Korea  ?\n",
      "Score 0.4294359000773963 \n",
      "\n",
      "Article ID  17574\n",
      "Question: What scandal led to President Park's impeachment?\n",
      "Answer: SEOUL, South Korea  ?\n",
      "Score 0.4610361928916077 \n",
      "\n",
      "Article ID  17574\n",
      "Question: What is the name of Samsung's vice chairman?\n",
      "No answer found\n",
      "Article ID  17574\n",
      "Question: What is the name of the special prosecutor investigating the corruption scandal?\n",
      "Answer: SEOUL, South Korea  ?\n",
      "Score 0.4376736307642736 \n",
      "\n",
      "Article ID  17574\n",
      "Question: Who is on trial at the Constitutional Court?\n",
      "Answer: Allegations that Ms. Park helped Ms. Choi extort millions in bribes from Samsung and other companies are at the heart of the corruption scandal that led to the National Assembly?s vote to impeach her last month.\n",
      "Score 0.47337877099319725 \n",
      "\n",
      "Article ID  17574\n",
      "Question: What is the name of the special prosecutor's office spokesman?\n",
      "Answer: ?\n",
      "Score 0.4003091553243662 \n",
      "\n",
      "Article ID  17574\n",
      "Question: What charges were filed against Ms. Choi by state prosecutors?\n",
      "Answer: It asked the National Assembly to file a perjury complaint against Mr. Lee, which would authorize the special prosecutor to open an investigation of that charge.\n",
      "Score 0.3410865843748847 \n",
      "\n",
      "Article ID  17574\n",
      "Question: What organization's support was crucial for the merger of two Samsung affiliates?\n",
      "Answer: 3 million to a winter sports program for young athletes that Ms. Choi and her nephew ran.\n",
      "Score 0.4202492249757479 \n",
      "\n",
      "Article ID  17574\n",
      "Question: What amount did Samsung contribute to Ms. Choi's winter sports program?\n",
      "Answer: Samsung gave the largest donations to Ms. Choi?s foundations, totaling $17 million.\n",
      "Score 0.5287991060321675 \n",
      "\n",
      "Mean Reciprocal Rank (MRR) with spaCy word vector: 0\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of TextMatchingUtility\n",
    "tm_utility = TextMatchingUtility(news_dataset)\n",
    "\n",
    "# Create an instance of TestUtility\n",
    "test_utility = TestUtility(tm_utility, test_questions)\n",
    "\n",
    "# Test with spaCy word vector and article id\n",
    "article_id = 17574  # Replace with appropriate article ID\n",
    "mrr = test_utility.evaluate_mrr(article_id, word_vector='tf-idf')\n",
    "print(\"Mean Reciprocal Rank (MRR) with spaCy word vector:\", mrr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58aafc9f-6c3e-4a0b-9353-7a7571b58098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article ID  17574\n",
      "Question: Who is the vice chairman of Samsung?\n",
      "Answer: The reference on Wednesday to possible perjury charges against Mr. Lee stemmed from that testimony.\n",
      "Score 0.8260947373005356 \n",
      "\n",
      "Article ID  17574\n",
      "Question: Who is the de facto head of Samsung being questioned for bribery?\n",
      "Answer: SEOUL, South Korea  ?\n",
      "Score 0.8305692713275632 \n",
      "\n",
      "Article ID  17574\n",
      "Question: What scandal led to President Park's impeachment?\n",
      "Answer: SEOUL, South Korea  ?\n",
      "Score 0.8743210249797997 \n",
      "\n",
      "Article ID  17574\n",
      "Question: What is the name of Samsung's vice chairman?\n",
      "Answer: The reference on Wednesday to possible perjury charges against Mr. Lee stemmed from that testimony.\n",
      "Score 0.8286177302079136 \n",
      "\n",
      "Article ID  17574\n",
      "Question: What is the name of the special prosecutor investigating the corruption scandal?\n",
      "Answer: In November, state prosecutors indicted Ms. Choi on charges of coercing 53 big businesses, including Samsung, to contribute $69 million to her two foundations.\n",
      "Score 0.8390553654061038 \n",
      "\n",
      "Article ID  17574\n",
      "Question: Who is on trial at the Constitutional Court?\n",
      "Answer: Allegations that Ms. Park helped Ms. Choi extort millions in bribes from Samsung and other companies are at the heart of the corruption scandal that led to the National Assembly?s vote to impeach her last month.\n",
      "Score 0.8570156087570253 \n",
      "\n",
      "Article ID  17574\n",
      "Question: What is the name of the special prosecutor's office spokesman?\n",
      "Answer: The reference on Wednesday to possible perjury charges against Mr. Lee stemmed from that testimony.\n",
      "Score 0.8962989498359315 \n",
      "\n",
      "Article ID  17574\n",
      "Question: What charges were filed against Ms. Choi by state prosecutors?\n",
      "Answer: ?\n",
      "Score 0.8465718952186104 \n",
      "\n",
      "Article ID  17574\n",
      "Question: What organization's support was crucial for the merger of two Samsung affiliates?\n",
      "Answer: 3 million to a winter sports program for young athletes that Ms. Choi and her nephew ran.\n",
      "Score 0.8686864250771845 \n",
      "\n",
      "Article ID  17574\n",
      "Question: What amount did Samsung contribute to Ms. Choi's winter sports program?\n",
      "Answer: The special prosecutor, which took over the investigations from the state prosecutors last month, has been looking into possible bribery charges against not only Ms. Park but the businesses, particularly Samsung.\n",
      "Score 0.8730683278283571 \n",
      "\n",
      "Mean Reciprocal Rank (MRR) with spaCy word vector: 0\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of TextMatchingUtility\n",
    "tm_utility = TextMatchingUtility(news_dataset)\n",
    "\n",
    "# Create an instance of TestUtility\n",
    "test_utility = TestUtility(tm_utility, test_questions)\n",
    "\n",
    "# Test with spaCy word vector and article id\n",
    "article_id = 17574  # Replace with appropriate article ID\n",
    "mrr = test_utility.evaluate_mrr(article_id, word_vector='spaCy')\n",
    "print(\"Mean Reciprocal Rank (MRR) with spaCy word vector:\", mrr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33c730cb-d95a-41a8-a5bb-f68bc1e1e432",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionAnsweringSystem:\n",
    "    def __init__(self, model_name='deepset/bert-base-cased-squad2'):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "    def answer_question(self, question, article_id):\n",
    "        passage = news_dataset.loc[news_dataset['id'] == article_id, 'processed_article'].iloc[0]\n",
    "        inputs = self.tokenizer.encode_plus(question, passage, return_tensors='pt', max_length=512, truncation=True, truncation_strategy='longest_first')\n",
    "\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        start_logits = outputs.start_logits\n",
    "        end_logits = outputs.end_logits\n",
    "\n",
    "        start_index = torch.argmax(start_logits)\n",
    "        end_index = torch.argmax(end_logits) + 1\n",
    "\n",
    "        input_tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "        answer_tokens = input_ids[0][start_index:end_index]\n",
    "\n",
    "        for i, token in enumerate(answer_tokens):\n",
    "            if token == self.tokenizer.cls_token_id:\n",
    "                start_index += 1\n",
    "            elif token == self.tokenizer.sep_token_id:\n",
    "                end_index -= 1\n",
    "        answer_tokens = input_ids[0][start_index:end_index]\n",
    "\n",
    "        answer = self.tokenizer.decode(answer_tokens)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cd8918a-0e4d-4477-a33a-b4ca9dd6bfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestUtility:\n",
    "    def __init__(self, test_questions):\n",
    "        self.test_questions = test_questions\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy(test_questions, predicted_labels):\n",
    "        if len(test_questions) != len(predicted_labels):\n",
    "            raise ValueError(\"Length of test_questions and predicted_labels must be the same.\")\n",
    "\n",
    "        correct = 0\n",
    "        total = len(test_questions)\n",
    "        for i in range(total):\n",
    "            correct_answer = test_questions[i]['answer'].lower()\n",
    "            predicted_label = predicted_labels[i].lower()\n",
    "            correct_answer = correct_answer.replace(\" \", \"\")\n",
    "            print(correct_answer,predicted_label)\n",
    "            if correct_answer in predicted_label:\n",
    "                correct += 1\n",
    "        if total == 0:\n",
    "            return 0  # Return 0 if there are no test questions\n",
    "        return correct / total\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_reciprocal_rank(test_questions, predicted_labels):\n",
    "        rr_sum = 0\n",
    "        total = len(test_questions)\n",
    "        for i in range(total):\n",
    "            correct_answer = test_questions[i]['answer'].lower()\n",
    "            predicted_labels_lower = predicted_labels[i].lower()\n",
    "            correct_answer = correct_answer.replace(\" \", \"\")\n",
    "            if correct_answer in predicted_labels_lower:\n",
    "                rr_sum += 1 / (predicted_labels_lower.index(correct_answer) + 1)\n",
    "        return rr_sum / total if total != 0 else 0\n",
    "\n",
    "    def evaluate_performance(self, predicted_labels):\n",
    "        acc = self.accuracy(self.test_questions, predicted_labels)\n",
    "        mrr = self.mean_reciprocal_rank(self.test_questions, predicted_labels)\n",
    "\n",
    "        print(\"Accuracy:\", acc)\n",
    "        print(\"Mean Reciprocal Rank:\", mrr)\n",
    "\n",
    "    def get_predicted_labels(self, article_id, qa_model):\n",
    "        start_time = time.time()\n",
    "        answers = []\n",
    "        print_query = []\n",
    "        for index, question in enumerate(self.test_questions):\n",
    "            query = qa_model.answer_question(question['query'], article_id) \n",
    "            predicted_labels = query.replace(\" \", \"\")\n",
    "            answers.append(predicted_labels)\n",
    "            print_query.append(query)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(\"Execution time:\", execution_time, \"seconds\")\n",
    "        return answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71f27a62-f22e-4154-bfbb-3234aa462caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 4053a733-fbb5-406f-958b-8b3e34f1f54c)')' thrown while requesting HEAD https://huggingface.co/deepset/bert-base-cased-squad2/resolve/main/tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fdb725f243b41eda0ce2241f894d686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is the vice chairman of Samsung?\n",
      "Answer: $ 69 million\n"
     ]
    }
   ],
   "source": [
    "article_id = 17574\n",
    "question = \"Who is the vice chairman of Samsung?\"\n",
    "\n",
    "qa_system = QuestionAnsweringSystem()\n",
    "answer = qa_system.answer_question(question, article_id)\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "341fcb0b-bca3-4074-9ac4-848cbedc72fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': 'Who is the vice chairman of Samsung?', 'answer': 'Jay Y. Lee'},\n",
       " {'query': 'Who is the de facto head of Samsung being questioned for bribery?',\n",
       "  'answer': 'Jay Y. Lee'},\n",
       " {'query': \"What scandal led to President Park's impeachment?\",\n",
       "  'answer': 'corruption scandal'},\n",
       " {'query': \"What is the name of Samsung's vice chairman?\",\n",
       "  'answer': 'Jay Y. Lee'},\n",
       " {'query': 'What is the name of the special prosecutor investigating the corruption scandal?',\n",
       "  'answer': '[SEP]'},\n",
       " {'query': 'Who is on trial at the Constitutional Court?',\n",
       "  'answer': 'Ms. Park'},\n",
       " {'query': \"What is the name of the special prosecutor's office spokesman?\",\n",
       "  'answer': 'Lee'},\n",
       " {'query': 'What charges were filed against Ms. Choi by state prosecutors?',\n",
       "  'answer': 'coercing 53 big businesses'},\n",
       " {'query': \"What organization's support was crucial for the merger of two Samsung affiliates?\",\n",
       "  'answer': 'National Pension Service'},\n",
       " {'query': \"What amount did Samsung contribute to Ms. Choi's winter sports program?\",\n",
       "  'answer': '$1.3 million'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "700bbc91-f8ad-4dc0-9b9d-0594d05689bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 23.92566180229187 seconds\n",
      "jayy.lee $69million\n",
      "jayy.lee coercing53bigbusinesses,includingsamsung,contribute$69million\n",
      "corruptionscandal coercing53bigbusinesses,includingsamsung,contribute$69million\n",
      "jayy.lee coercing53bigbusinesses,includingsamsung,contribute$69million\n",
      "[sep] coercing53bigbusinesses,includingsamsung,contribute$69million\n",
      "ms.park $69million\n",
      "lee coercing53bigbusinesses,includingsamsung,contribute$69million\n",
      "coercing53bigbusinesses coercing53bigbusinesses,includingsamsung,contribute$69million\n",
      "nationalpensionservice coercing53bigbusinesses,includingsamsung,contribute$69million\n",
      "$1.3million $69million\n",
      "Accuracy: 0.1\n",
      "Mean Reciprocal Rank: 0.1\n"
     ]
    }
   ],
   "source": [
    "# Instantiate TestUtility object with test questions\n",
    "test_utility = TestUtility(test_questions)\n",
    "\n",
    "# Call get_predicted_labels method to obtain predicted labels\n",
    "article_id = 17574\n",
    "\n",
    "predicted_labels = test_utility.get_predicted_labels(article_id, qa_system)\n",
    "\n",
    "# Now you can pass these predicted labels to evaluate_performance method to evaluate performance\n",
    "test_utility.evaluate_performance(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aee88b72-d7cc-43fc-a614-935f53138d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ROBERTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73b1bd36-6320-49b9-bad1-64e28679399a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0fb0c0f216d40b983bd50cc42e85cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/79.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db657dfb2b274bafa4eb5caec2b12593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565b974b2a944b758ad6aef4edc3eb41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3af59ad27f4b08be7aa8dcce1da649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0489016451474d5e9332e3cf4fb7d3e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd0d39dc073c49698e96a44d746cb515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is the vice chairman of Samsung?\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "article_id = 17574\n",
    "question = \"Who is the vice chairman of Samsung?\"\n",
    "\n",
    "qa_system_roberta = QuestionAnsweringSystem(model_name= \"deepset/roberta-base-squad2\")\n",
    "answer = qa_system_roberta.answer_question(question, article_id)\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0a31a0b-e968-4e60-ba0e-d1eecd51e22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jay Y. Lee', 'Jay Y. Lee', 'corruption scandal', 'Jay Y. Lee', '[SEP]', 'Ms. Park', 'Lee', 'coercing 53 big businesses', 'National Pension Service', '$1.3 million']\n"
     ]
    }
   ],
   "source": [
    "true_labels = [item['answer'] for item in test_questions]\n",
    "print(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2eeef873-90ae-4a73-a23f-c9f6edd8e838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 23.97845196723938 seconds\n",
      "['Jay Y. Lee', 'Jay Y. Lee', 'corruption scandal', 'Jay Y. Lee', '[SEP]', 'Ms. Park', 'Lee', 'coercing 53 big businesses', 'National Pension Service', '$1.3 million']\n",
      "jayy.lee \n",
      "jayy.lee mr.lee\n",
      "corruptionscandal corruption\n",
      "jayy.lee \n",
      "[sep] \n",
      "ms.park \n",
      "lee mr.leesaid\n",
      "coercing53bigbusinesses \n",
      "nationalpensionservice \n",
      "$1.3million $1.3million\n",
      "Accuracy: 0.2\n",
      "Mean Reciprocal Rank: 0.125\n",
      "Execution time: 23.856518745422363 seconds\n",
      "['Jay Y. Lee', 'Jay Y. Lee', 'corruption scandal', 'Jay Y. Lee', '[SEP]', 'Ms. Park', 'Lee', 'coercing 53 big businesses', 'National Pension Service', '$1.3 million']\n",
      "jayy.lee \n",
      "jayy.lee mr.lee\n",
      "corruptionscandal corruption\n",
      "jayy.lee \n",
      "[sep] \n",
      "ms.park \n",
      "lee mr.leesaid\n",
      "coercing53bigbusinesses \n",
      "nationalpensionservice \n",
      "$1.3million $1.3million\n",
      "Accuracy: 0.2\n",
      "Mean Reciprocal Rank: 0.125\n"
     ]
    }
   ],
   "source": [
    "# Instantiate TestUtility object with test questions\n",
    "test_utility = TestUtility(test_questions)\n",
    "\n",
    "# Call get_predicted_labels method to obtain predicted labels\n",
    "article_id = 17574\n",
    "\n",
    "predicted_labels = test_utility.get_predicted_labels(article_id, qa_system_roberta)\n",
    "\n",
    "print(true_labels)\n",
    "test_utility.evaluate_performance(predicted_labels)# Instantiate TestUtility object with test questions\n",
    "test_utility = TestUtility(test_questions)\n",
    "\n",
    "# Call get_predicted_labels method to obtain predicted labels\n",
    "article_id = 17574\n",
    "\n",
    "predicted_labels = test_utility.get_predicted_labels(article_id, qa_system_roberta)\n",
    "\n",
    "print(true_labels)\n",
    "test_utility.evaluate_performance(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dc61bfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers==4.30.2 in /opt/anaconda3/envs/py37/lib/python3.7/site-packages (4.30.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/anaconda3/envs/py37/lib/python3.7/site-packages (from transformers==4.30.2) (0.16.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/anaconda3/envs/py37/lib/python3.7/site-packages (from transformers==4.30.2) (0.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/py37/lib/python3.7/site-packages (from transformers==4.30.2) (1.21.6)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/py37/lib/python3.7/site-packages (from transformers==4.30.2) (3.12.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/anaconda3/envs/py37/lib/python3.7/site-packages (from transformers==4.30.2) (0.4.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/py37/lib/python3.7/site-packages (from transformers==4.30.2) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/py37/lib/python3.7/site-packages (from transformers==4.30.2) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/py37/lib/python3.7/site-packages (from transformers==4.30.2) (2024.4.16)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/py37/lib/python3.7/site-packages (from transformers==4.30.2) (4.66.2)\n",
      "Requirement already satisfied: importlib-metadata in /opt/anaconda3/envs/py37/lib/python3.7/site-packages (from transformers==4.30.2) (4.11.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/py37/lib/python3.7/site-packages (from transformers==4.30.2) (2.31.0)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/py37/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (2023.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/py37/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (4.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/envs/py37/lib/python3.7/site-packages (from importlib-metadata->transformers==4.30.2) (3.11.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/py37/lib/python3.7/site-packages (from requests->transformers==4.30.2) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/py37/lib/python3.7/site-packages (from requests->transformers==4.30.2) (1.26.18)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/py37/lib/python3.7/site-packages (from requests->transformers==4.30.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/py37/lib/python3.7/site-packages (from requests->transformers==4.30.2) (3.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers==4.30.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "37c901fa-2ac1-457f-b5f0-9215604fb3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c6f9c39b-a78e-4bcf-8679-7640fd944565)')' thrown while requesting HEAD https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89cb163aa16349bc9a69f73b84aec1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:  52%|#####2    | 703M/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mtimeout\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                 \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_error_catcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;31m# StringIO doesn't like amt=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1070\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mtimeout\u001b[0m: The read operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    815\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    592\u001b[0m                         \u001b[0;31m# Content-Length are caught.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp_bytes_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength_remaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;31m# there is yet no clean way to get at it from this context.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mReadTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Read timed out.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nl/82xzs_994px62t4_4mv_5lnh0000gn/T/ipykernel_27963/3297235988.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertForQuestionAnswering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForQuestionAnswering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-large-uncased-whole-word-masking-finetuned-squad'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-large-uncased-whole-word-masking-finetuned-squad'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2492\u001b[0m                         \u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcommit_hash\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2493\u001b[0m                     }\n\u001b[0;32m-> 2494\u001b[0;31m                     \u001b[0mresolved_archive_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcached_file_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2496\u001b[0m                     \u001b[0;31m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m         )\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1368\u001b[0m                 \u001b[0mresume_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m                 \u001b[0mexpected_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpected_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m             )\n\u001b[1;32m   1372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetEffectiveLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTSET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     )\n\u001b[0;32m--> 541\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1024\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    820\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mContentDecodingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mReadTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mRequestsSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out."
     ]
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad', resume_download=True)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad', resume_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c557d3-3758-407e-8d81-3c5d1c67de70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class QuestionAnsweringModel:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def tokenize(self, question, answer_text):\n",
    "        '''\n",
    "        Tokenizes the input question and answer_text and sets the segment IDs.\n",
    "        '''\n",
    "        # Apply the tokenizer to the encode text, treating them as a question, answer_text pair.\n",
    "        input_ids = self.tokenizer.encode(question, answer_text, max_length=512, truncation=True, truncation_strategy='only_second')\n",
    "\n",
    "        # Report how long the input sequence is.\n",
    "        # print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
    "\n",
    "        # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "        sep_index = input_ids.index(self.tokenizer.sep_token_id)\n",
    "\n",
    "        # The number of segment A tokens includes the [SEP] token itself.\n",
    "        num_seg_a = sep_index + 1\n",
    "\n",
    "        # The remainder are segment B.\n",
    "        num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "        # Construct the list of 0s and 1s.\n",
    "        segment_ids = [0] * num_seg_a + [1] * num_seg_b\n",
    "\n",
    "        # There should be a segment_id for every input token.\n",
    "        assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "        return input_ids, segment_ids\n",
    "\n",
    "    def evaluate(self, input_ids, segment_ids):\n",
    "        '''\n",
    "        Evaluates the input question and answer_text using the model.\n",
    "        '''\n",
    "        # Run the question through the model.\n",
    "        model_scores = self.model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids])) \n",
    "        start_scores = model_scores.start_logits\n",
    "        end_scores = model_scores.end_logits\n",
    "\n",
    "        return start_scores, end_scores\n",
    "\n",
    "    def reconstruct_answer(self, input_ids, start_scores, end_scores):\n",
    "        '''\n",
    "        Reconstructs the answer from the model's output.\n",
    "        '''\n",
    "        # Find the tokens with the highest `start` and `end` scores.\n",
    "        answer_start = torch.argmax(start_scores)\n",
    "        answer_end = torch.argmax(end_scores)\n",
    "\n",
    "        # Get the string versions of the input tokens.\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "        # Start with the first token.\n",
    "        answer = tokens[answer_start]\n",
    "\n",
    "        # Select the remaining answer tokens and join them with whitespace.\n",
    "        for i in range(answer_start + 1, answer_end + 1):\n",
    "            # If it's a subword token, then recombine it with the previous token.\n",
    "            if tokens[i][0:2] == '##':\n",
    "                answer += tokens[i][2:]\n",
    "            # Otherwise, add a space then the token.\n",
    "            else:\n",
    "                answer += ' ' + tokens[i]\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def answer_question(self, question, article_id):\n",
    "        '''\n",
    "        Takes a `question` string and an `answer_text` string (which contains the\n",
    "        answer), and identifies the words within the `answer_text` that are the\n",
    "        answer. Prints them out.\n",
    "        '''\n",
    "        #get article\n",
    "        answer_text = news_dataset.loc[news_dataset['id'] == article_id, 'processed_article'].iloc[0]\n",
    "\n",
    "        # Tokenize\n",
    "        input_ids, segment_ids = self.tokenize(question, answer_text)\n",
    "\n",
    "        # Evaluate\n",
    "        start_scores, end_scores = self.evaluate(input_ids, segment_ids)\n",
    "\n",
    "        # Reconstruct Answer\n",
    "        answer = self.reconstruct_answer(input_ids, start_scores, end_scores)\n",
    "\n",
    "        return answer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201ef03f-a4e1-4ba7-bb6d-e8a798f1c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "qa_model = QuestionAnsweringModel(model, tokenizer)\n",
    "\n",
    "# Define your question and article ID\n",
    "question = \"Who is the vice chairman of Samsung?\"\n",
    "article_id =17574 \n",
    "\n",
    "# Call the answer_question method to get the answer\n",
    "answer = qa_model.answer_question(question, article_id)\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed93e50b-267d-428a-95f2-cb6203bae6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_labels(test_questions):\n",
    "    start_time = time.time()\n",
    "    answers = []\n",
    "    article_id = 17574\n",
    "    for index, question in enumerate(test_questions):\n",
    "        query = qa_model.answer_question(question['query'], article_id)\n",
    "        predicted_labels = query.replace(\" \", \"\")\n",
    "        answers.append(predicted_labels)\n",
    "    return answers\n",
    "predicted_labels = predicted_labels(test_questions)\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dfd69a-5569-4eef-b04f-23460682ef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcca66eb-f729-49ff-935b-6aaf463a9d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate TestUtility object with test questions\n",
    "test_utility = TestUtility(test_questions)\n",
    "\n",
    "# Call get_predicted_labels method to obtain predicted labels\n",
    "article_id = 17574\n",
    "\n",
    "predicted_labels = test_utility.get_predicted_labels(article_id, qa_model)\n",
    "\n",
    "test_utility.evaluate_performance(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5181aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d3a91b4",
   "metadata": {},
   "source": [
    "## B. References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7553c1",
   "metadata": {},
   "source": [
    "## C. Appendix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
