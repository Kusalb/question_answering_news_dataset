{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6730fecf",
   "metadata": {},
   "source": [
    "# Assignment 2 \n",
    "### \\<name1> \\<id1>\n",
    "### \\<name2> \\<id2>\n",
    "### \\<name3> \\<id3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07279f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 读取数据集\n",
    "df = pd.read_csv('news_dataset.csv', encoding='ISO-8859-1')\n",
    "# 随机抽样\n",
    "df_sample = df.sample(n=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ed72477",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yulun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yulun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yulun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# 首次使用需要下载停用词列表和WordNet词形还原器资源\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# 加载数据集\n",
    "# 假设df是已经加载好的pandas DataFrame，其中包含一个名为'article'的文本列\n",
    "\n",
    "# 初始化词形还原器\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# 停用词列表\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 将文本转换为小写\n",
    "    text = text.lower()\n",
    "    # 去除特殊字符和数字\n",
    "    text = re.sub(r'\\W+|\\d+', ' ', text)\n",
    "    # 分词\n",
    "    tokens = word_tokenize(text)\n",
    "    # 去除停用词并进行词形还原\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    # 将处理后的单词合并回文本\n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "    return processed_text\n",
    "\n",
    "# 应用预处理函数到文章列\n",
    "df['processed_article'] = df['article'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3425b5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    PARIS  ?   When the Islamic State was about to...\n",
      "1    Angels are everywhere in the Mu?iz family?s ap...\n",
      "2    Finally. The Second Avenue subway opened in Ne...\n",
      "3    WASHINGTON  ?   It?s   or   time for Republica...\n",
      "4    For Megyn Kelly, the shift from Fox News to NB...\n",
      "Name: article, dtype: object\n",
      "0    paris islamic state driven ancient city palmyr...\n",
      "1    angel everywhere mu iz family apartment bronx ...\n",
      "2    finally second avenue subway opened new york c...\n",
      "3    washington time republican tumultuous decade s...\n",
      "4    megyn kelly shift fox news nbc host daily dayt...\n",
      "Name: processed_article, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 查看预处理后的数据\n",
    "print(df['article'].head())\n",
    "print(df['processed_article'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "914eeb89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of         id               author        date  year month         topic  \\\n",
       "0    17307       Marlise Simons   1/01/2017  2017     1  architecture   \n",
       "1    17292          Andy Newman  31/12/2016  2016    12           art   \n",
       "2    17298  Emma G. Fitzsimmons   2/01/2017  2017     1      business   \n",
       "3    17311           Carl Hulse   3/01/2017  2017     1      business   \n",
       "4    17339        Jim Rutenberg   5/01/2017  2017     1      business   \n",
       "..     ...                  ...         ...   ...   ...           ...   \n",
       "995  18460        Gerry Mullany  14/03/2017  2017     3     accidents   \n",
       "996  18461           Rory Smith  10/02/2017  2017     2        sports   \n",
       "997  18462           Jack Ewing   9/02/2017  2017     2      business   \n",
       "998  18463       Scott Cacciola  10/02/2017  2017     2        sports   \n",
       "999  18465          Sam Roberts  10/02/2017  2017     2     lifestyle   \n",
       "\n",
       "                                               article  \\\n",
       "0    PARIS  ?   When the Islamic State was about to...   \n",
       "1    Angels are everywhere in the Mu?iz family?s ap...   \n",
       "2    Finally. The Second Avenue subway opened in Ne...   \n",
       "3    WASHINGTON  ?   It?s   or   time for Republica...   \n",
       "4    For Megyn Kelly, the shift from Fox News to NB...   \n",
       "..                                                 ...   \n",
       "995  HONG KONG  ?   Hundreds of pilot whales that s...   \n",
       "996  NICE, France  ?     Riv?re accepts the complim...   \n",
       "997  FRANKFURT  ?   Germans who never really warmed...   \n",
       "998  Charles Oakley has strong feelings about compe...   \n",
       "999  Hans Rosling, a Swedish doctor who transformed...   \n",
       "\n",
       "                                     processed_article  \n",
       "0    paris islamic state driven ancient city palmyr...  \n",
       "1    angel everywhere mu iz family apartment bronx ...  \n",
       "2    finally second avenue subway opened new york c...  \n",
       "3    washington time republican tumultuous decade s...  \n",
       "4    megyn kelly shift fox news nbc host daily dayt...  \n",
       "..                                                 ...  \n",
       "995  hong kong hundred pilot whale swam shallow new...  \n",
       "996  nice france riv accepts compliment reject comp...  \n",
       "997  frankfurt german never really warmed euro may ...  \n",
       "998  charles oakley strong feeling comped ticket fr...  \n",
       "999  han rosling swedish doctor transformed statist...  \n",
       "\n",
       "[1000 rows x 8 columns]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "480e6ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting neuralcoref\n",
      "  Using cached neuralcoref-4.0.tar.gz (368 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from neuralcoref) (1.26.4)\n",
      "Collecting boto3 (from neuralcoref)\n",
      "  Using cached boto3-1.34.80-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from neuralcoref) (2.31.0)\n",
      "Requirement already satisfied: spacy>=2.1.0 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from neuralcoref) (3.7.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2024.2.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (4.66.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (2.6.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (69.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yulun\\appdata\\roaming\\python\\python311\\site-packages (from spacy>=2.1.0->neuralcoref) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (3.3.0)\n",
      "Collecting botocore<1.35.0,>=1.34.80 (from boto3->neuralcoref)\n",
      "  Using cached botocore-1.34.80-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->neuralcoref)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->neuralcoref)\n",
      "  Using cached s3transfer-0.10.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\yulun\\appdata\\roaming\\python\\python311\\site-packages (from botocore<1.35.0,>=1.34.80->boto3->neuralcoref) (2.8.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.1.0->neuralcoref) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.1.0->neuralcoref) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.1.0->neuralcoref) (4.9.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.1.0->neuralcoref) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.1.0->neuralcoref) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\yulun\\appdata\\roaming\\python\\python311\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy>=2.1.0->neuralcoref) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy>=2.1.0->neuralcoref) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy>=2.1.0->neuralcoref) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yulun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy>=2.1.0->neuralcoref) (2.1.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yulun\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.80->boto3->neuralcoref) (1.16.0)\n",
      "Using cached boto3-1.34.80-py3-none-any.whl (139 kB)\n",
      "Using cached botocore-1.34.80-py3-none-any.whl (12.1 MB)\n",
      "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Using cached s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
      "Building wheels for collected packages: neuralcoref\n",
      "  Building wheel for neuralcoref (pyproject.toml): started\n",
      "  Building wheel for neuralcoref (pyproject.toml): finished with status 'error'\n",
      "Failed to build neuralcoref\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for neuralcoref (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [25 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-311\n",
      "      creating build\\lib.win-amd64-cpython-311\\neuralcoref\n",
      "      copying neuralcoref\\file_utils.py -> build\\lib.win-amd64-cpython-311\\neuralcoref\n",
      "      copying neuralcoref\\__init__.py -> build\\lib.win-amd64-cpython-311\\neuralcoref\n",
      "      creating build\\lib.win-amd64-cpython-311\\neuralcoref\\tests\n",
      "      copying neuralcoref\\tests\\test_neuralcoref.py -> build\\lib.win-amd64-cpython-311\\neuralcoref\\tests\n",
      "      copying neuralcoref\\tests\\__init__.py -> build\\lib.win-amd64-cpython-311\\neuralcoref\\tests\n",
      "      creating build\\lib.win-amd64-cpython-311\\neuralcoref\\train\n",
      "      copying neuralcoref\\train\\algorithm.py -> build\\lib.win-amd64-cpython-311\\neuralcoref\\train\n",
      "      copying neuralcoref\\train\\compat.py -> build\\lib.win-amd64-cpython-311\\neuralcoref\\train\n",
      "      copying neuralcoref\\train\\conllparser.py -> build\\lib.win-amd64-cpython-311\\neuralcoref\\train\n",
      "      copying neuralcoref\\train\\dataset.py -> build\\lib.win-amd64-cpython-311\\neuralcoref\\train\n",
      "      copying neuralcoref\\train\\document.py -> build\\lib.win-amd64-cpython-311\\neuralcoref\\train\n",
      "      copying neuralcoref\\train\\evaluator.py -> build\\lib.win-amd64-cpython-311\\neuralcoref\\train\n",
      "      copying neuralcoref\\train\\learn.py -> build\\lib.win-amd64-cpython-311\\neuralcoref\\train\n",
      "      copying neuralcoref\\train\\model.py -> build\\lib.win-amd64-cpython-311\\neuralcoref\\train\n",
      "      copying neuralcoref\\train\\utils.py -> build\\lib.win-amd64-cpython-311\\neuralcoref\\train\n",
      "      copying neuralcoref\\train\\__init__.py -> build\\lib.win-amd64-cpython-311\\neuralcoref\\train\n",
      "      running build_ext\n",
      "      building 'neuralcoref.neuralcoref' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for neuralcoref\n",
      "ERROR: Could not build wheels for neuralcoref, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "pip install --use-pep517 neuralcoref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4286349",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neuralcoref'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mneuralcoref\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 加载SpaCy的英文模型\u001b[39;00m\n\u001b[0;32m      5\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'neuralcoref'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import neuralcoref\n",
    "\n",
    "# 加载SpaCy的英文模型\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# 将NeuralCoref添加到SpaCy的管道中\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "# 示例文本\n",
    "text = \"The de facto leader, Jay Y. Lee, the vice chairman of Samsung, was convicted. He appealed the conviction.\"\n",
    "\n",
    "# 使用SpaCy处理文本\n",
    "doc = nlp(text)\n",
    "\n",
    "# 打印出指代消解后的结果\n",
    "print(doc._.coref_resolved)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df2aac6",
   "metadata": {},
   "source": [
    "## A. Tasks as specified for your team structure\n",
    "\n",
    "**One headings for each task.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3a91b4",
   "metadata": {},
   "source": [
    "## B. References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7553c1",
   "metadata": {},
   "source": [
    "## C. Appendix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
