{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6730fecf",
   "metadata": {},
   "source": [
    "# Assignment 2 \n",
    "### Kusal Bista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "fb617560-ebfe-47a0-a5ee-fb6a0ec56589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for reading data\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Libraries for pre-processing\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk import tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Libraries for information retrieval\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.lang.en import English\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Libraries for data analysis\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "20cde6e7-8e19-4357-8ade-60114da4174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download en_core_web_md\n",
    "# !pip install tabulate\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "5d0d6e5f-384d-47c7-b959-51604a6bc35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\a1881044\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\a1881044\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\a1881044\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\a1881044\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd8a509-a9f1-405e-8d0b-6dc96e679322",
   "metadata": {},
   "source": [
    "### 1 Reading dataset and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "c1474a82-79ad-4c8d-8bae-e4d4056d2860",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = pd.read_csv('news_dataset.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "de28f0fb-9811-4b2d-96f4-63ff46825a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>topic</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17307</td>\n",
       "      <td>Marlise Simons</td>\n",
       "      <td>1/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>architecture</td>\n",
       "      <td>PARIS  ?   When the Islamic State was about to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17292</td>\n",
       "      <td>Andy Newman</td>\n",
       "      <td>31/12/2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>art</td>\n",
       "      <td>Angels are everywhere in the Mu?iz family?s ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17298</td>\n",
       "      <td>Emma G. Fitzsimmons</td>\n",
       "      <td>2/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>Finally. The Second Avenue subway opened in Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17311</td>\n",
       "      <td>Carl Hulse</td>\n",
       "      <td>3/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>WASHINGTON  ?   It?s   or   time for Republica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17339</td>\n",
       "      <td>Jim Rutenberg</td>\n",
       "      <td>5/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>For Megyn Kelly, the shift from Fox News to NB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id               author        date  year month         topic  \\\n",
       "0  17307       Marlise Simons   1/01/2017  2017     1  architecture   \n",
       "1  17292          Andy Newman  31/12/2016  2016    12           art   \n",
       "2  17298  Emma G. Fitzsimmons   2/01/2017  2017     1      business   \n",
       "3  17311           Carl Hulse   3/01/2017  2017     1      business   \n",
       "4  17339        Jim Rutenberg   5/01/2017  2017     1      business   \n",
       "\n",
       "                                             article  \n",
       "0  PARIS  ?   When the Islamic State was about to...  \n",
       "1  Angels are everywhere in the Mu?iz family?s ap...  \n",
       "2  Finally. The Second Avenue subway opened in Ne...  \n",
       "3  WASHINGTON  ?   It?s   or   time for Republica...  \n",
       "4  For Megyn Kelly, the shift from Fox News to NB...  "
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "e04327fa-2d19-4ba8-9291-984303ca5ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # selecting 100 sample \n",
    "# sample_size = 100\n",
    "# if news_dataset.shape[0] >= sample_size:\n",
    "#     news_dataset_sm = news_dataset.sample(n=sample_size, random_state=42)  # Adjusting random_state for reproducibility\n",
    "#     news_dataset_sm.reset_index(drop=True, inplace=True) \n",
    "#     print(\"Sampled dataset shape:\", news_dataset_sm.shape)\n",
    "# else:\n",
    "#     print(\"Dataset size is less than the sample size. Cannot perform sampling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "573c3a0c-0f0f-4790-a794-9a0f87613737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 7 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   id       1000 non-null   int64 \n",
      " 1   author   994 non-null    object\n",
      " 2   date     1000 non-null   object\n",
      " 3   year     1000 non-null   object\n",
      " 4   month    1000 non-null   object\n",
      " 5   topic    1000 non-null   object\n",
      " 6   article  1000 non-null   object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 54.8+ KB\n"
     ]
    }
   ],
   "source": [
    "news_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b177652d-42ca-4e6f-ba8d-ad3a95c503b1",
   "metadata": {},
   "source": [
    "### 1.2 Handling missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "be0eccc9-7ea3-4155-be46-4373e2338b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing value\n",
      "id         0\n",
      "author     6\n",
      "date       0\n",
      "year       0\n",
      "month      0\n",
      "topic      0\n",
      "article    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing value\")\n",
    "print(news_dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "bf22242e-51d4-48a3-b91d-58d55ca2216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing value\n",
    "news_dataset['author'] = news_dataset['author'].fillna('No author')\n",
    "# checking missing value after handling missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "71e52309-3c2c-4899-b621-263597321d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After handling missing value\n",
      "id         0\n",
      "author     0\n",
      "date       0\n",
      "year       0\n",
      "month      0\n",
      "topic      0\n",
      "article    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"After handling missing value\")\n",
    "print(news_dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcbbed6-f57f-4686-891e-f2b9d6466935",
   "metadata": {},
   "source": [
    "### 1.3 Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "2e2c0aee-9743-46fc-8810-c8072892a0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(data):\n",
    "    # Define stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update([\"This\", \"The\", \"the\"])\n",
    "\n",
    "    s = \" \\[(?=.*\\d).*?\\]\" \n",
    "\n",
    "    # Lemmatization and removal of stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    result = []\n",
    "    for text in data:\n",
    "        # Clean text\n",
    "        # Remove non-ASCII characters\n",
    "        text = ''.join([char for char in text if ord(char) < 128])\n",
    "\n",
    "        # Remove multiple spaces\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        # Remove question mark problems\n",
    "        text = re.sub(r'(\\s\\?)',' ',text)\n",
    "        text = re.sub(r\"\\b\\?\\b\", \"\\'\", text)\n",
    "        text = re.sub(r\"(,\\?)\",\",\", text)\n",
    "        text = re.sub(r\"\\?+\", \"?\", text)\n",
    "        text = text.strip()\n",
    "\n",
    "        # Lemmatization and removal of stopwords\n",
    "        processed_text = \" \".join([lemmatizer.lemmatize(word) for word in re.sub(s, \"\", text).split() if word.lower() not in stop_words])\n",
    "\n",
    "        result.append(processed_text)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d773dd50-b4ea-441c-97ea-d077c2a98e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset['processed_article'] = pre_process(news_dataset['article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "820cfb36-4b63-4def-be79-4844cbec48a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>topic</th>\n",
       "      <th>article</th>\n",
       "      <th>processed_article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17307</td>\n",
       "      <td>Marlise Simons</td>\n",
       "      <td>1/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>architecture</td>\n",
       "      <td>PARIS  ?   When the Islamic State was about to...</td>\n",
       "      <td>PARIS Islamic State driven ancient city Palmyr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17292</td>\n",
       "      <td>Andy Newman</td>\n",
       "      <td>31/12/2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>art</td>\n",
       "      <td>Angels are everywhere in the Mu?iz family?s ap...</td>\n",
       "      <td>Angels everywhere Mu'iz family's apartment Bro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17298</td>\n",
       "      <td>Emma G. Fitzsimmons</td>\n",
       "      <td>2/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>Finally. The Second Avenue subway opened in Ne...</td>\n",
       "      <td>Finally. Second Avenue subway opened New York ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17311</td>\n",
       "      <td>Carl Hulse</td>\n",
       "      <td>3/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>WASHINGTON  ?   It?s   or   time for Republica...</td>\n",
       "      <td>WASHINGTON time Republicans. tumultuous decade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17339</td>\n",
       "      <td>Jim Rutenberg</td>\n",
       "      <td>5/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>For Megyn Kelly, the shift from Fox News to NB...</td>\n",
       "      <td>Megyn Kelly, shift Fox News NBC host daily day...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id               author        date  year month         topic  \\\n",
       "0  17307       Marlise Simons   1/01/2017  2017     1  architecture   \n",
       "1  17292          Andy Newman  31/12/2016  2016    12           art   \n",
       "2  17298  Emma G. Fitzsimmons   2/01/2017  2017     1      business   \n",
       "3  17311           Carl Hulse   3/01/2017  2017     1      business   \n",
       "4  17339        Jim Rutenberg   5/01/2017  2017     1      business   \n",
       "\n",
       "                                             article  \\\n",
       "0  PARIS  ?   When the Islamic State was about to...   \n",
       "1  Angels are everywhere in the Mu?iz family?s ap...   \n",
       "2  Finally. The Second Avenue subway opened in Ne...   \n",
       "3  WASHINGTON  ?   It?s   or   time for Republica...   \n",
       "4  For Megyn Kelly, the shift from Fox News to NB...   \n",
       "\n",
       "                                   processed_article  \n",
       "0  PARIS Islamic State driven ancient city Palmyr...  \n",
       "1  Angels everywhere Mu'iz family's apartment Bro...  \n",
       "2  Finally. Second Avenue subway opened New York ...  \n",
       "3  WASHINGTON time Republicans. tumultuous decade...  \n",
       "4  Megyn Kelly, shift Fox News NBC host daily day...  "
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "9da4692d-9186-48f2-b9ad-1c180a3272b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading spaCy model\n",
    "nlp_md = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "class TextMatchingUtility:\n",
    "    def __init__(self, entities, dataset, kb):\n",
    "        self.ner = entities  # Named Entity Recognition result\n",
    "        self.data = dataset  # Dataset\n",
    "        self.kb = kb  # Knowledge Base\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def preprocess_query(self, query):\n",
    "        \"\"\"\n",
    "        Preprocess the query by removing stopwords and lemmatizing.\n",
    "\n",
    "        Args:\n",
    "        query: Input query string.\n",
    "\n",
    "        Returns:\n",
    "        List containing preprocessed query.\n",
    "        \"\"\"\n",
    "        # Regular expression to match text patterns\n",
    "        s = \" \\[(?=.*\\d).*?\\]\"\n",
    "\n",
    "        # Removing stopwords and Lemmatization\n",
    "        stop_words = stopwords.words('english')\n",
    "        stop_words.extend([\"This\", \"The\", \"the\"])\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        result = [\" \".join([lemmatizer.lemmatize(word) for word in re.sub(s, \"\", query).split() if word not in stop_words])]\n",
    "        return result\n",
    "\n",
    "\n",
    "    def tf_idf_score(self, query, articles):\n",
    "        \"\"\"\n",
    "        Calculate TF-IDF similarity score between query and articles.\n",
    "\n",
    "        Args:\n",
    "        query: Preprocessed query.\n",
    "        articles: List of articles.\n",
    "\n",
    "        Returns:\n",
    "        Array of similarity scores.\n",
    "        \"\"\"          \n",
    "        vectorizer = TfidfVectorizer()\n",
    "        # Convert to word vector\n",
    "        articles_wv = vectorizer.fit_transform(articles)\n",
    "        # Convert to word vector\n",
    "        query_wv = vectorizer.transform([query]) \n",
    "        # Calculate similarity\n",
    "        similarities = cosine_similarity(query_wv, articles_wv)[0]\n",
    "        return similarities\n",
    "\n",
    "    def spacy_score(self, query, articles):\n",
    "        \"\"\"\n",
    "        Calculate SpaCy similarity score between query and articles.\n",
    "\n",
    "        Args:\n",
    "        query: Preprocessed query.\n",
    "        articles: List of articles.\n",
    "\n",
    "        Returns:\n",
    "        List of similarity scores.\n",
    "        \"\"\"\n",
    "        # Convert to word vector\n",
    "        query_nlp = nlp_md(str(query))\n",
    "        # Convert to word vector\n",
    "        articles_nlp = [nlp_md(article) for article in articles]\n",
    "        # Calculate similarity\n",
    "        similarities = [query_nlp.similarity(article_nlp) for article_nlp in articles_nlp]\n",
    "        return similarities\n",
    "    \n",
    "    def get_best_sentence(self, query, article_id, word_vector):\n",
    "        article = self.data.loc[self.data['id'] == article_id, 'processed_article'].iloc[0]\n",
    "        # Convert text into sentences\n",
    "        sentences_clean = tokenize.sent_tokenize(article)\n",
    "        # Calculate similarity\n",
    "        if word_vector == \"tf-idf\":\n",
    "            similarities = self.tf_idf_score(query, sentences_clean)\n",
    "        elif word_vector == \"spaCy\":\n",
    "            similarities = self.spacy_score(query, sentences_clean)\n",
    "    \n",
    "        # Get the maximum score index position\n",
    "        best_idx = np.array(similarities).argmax()\n",
    "        # Get the best score\n",
    "        best_score = max(similarities)\n",
    "        # Get original data\n",
    "        for j in range(len(self.data['id'])):\n",
    "            if self.data['id'][j] == article_id:\n",
    "                topic = self.data['article'][j]\n",
    "        sentences_topic = tokenize.sent_tokenize(topic)\n",
    "        answer = sentences_topic[best_idx]\n",
    "        print(\"The article id is \", article_id)  \n",
    "        print(\"Question \\n\", query)\n",
    "        if best_score < 0.3:\n",
    "            print(\"No answer found\")\n",
    "        else:\n",
    "            print(\"Answer: \\n\", answer)\n",
    "            print(\"The score is \", best_score,\"\\n\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "f99598df-8b99-466c-83cc-95a86618fdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article id is  17328\n",
      "Question \n",
      " What types of items are included in Jonathan Lethem's archive besides manuscripts and letters?\n",
      "Answer: \n",
      " ?I cannot imagine myself not sharing the skills and   talent that I have,?\n",
      "The score is  0.7597321053798365 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of TextMatchingUtility\n",
    "tm_utility = TextMatchingUtility(article_ner_dict, news_dataset, KB2)\n",
    "\n",
    "# # Sample question\n",
    "question = \"What types of items are included in Jonathan Lethem's archive besides manuscripts and letters?\"\n",
    "article_id = 17328\n",
    "top_results = tm_utility.get_best_sentence(question,article_id, word_vector ='spaCy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ba4fc0-1ba8-48a1-b172-20381d0b1762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3df2aac6",
   "metadata": {},
   "source": [
    "## A. Tasks as specified for your team structure\n",
    "\n",
    "**One headings for each task.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3a91b4",
   "metadata": {},
   "source": [
    "## B. References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7553c1",
   "metadata": {},
   "source": [
    "## C. Appendix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
